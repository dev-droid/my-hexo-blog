---
title: 阶段二核心（三）：DDPG训练启动——智能体“大脑”的深度架构与超参数工程
date: 2025-10-30 14:00:00
description: 详细解析用于海洋探鱼智能体控制的DDPG/TD3强化学习架构。深入剖析Actor-Critic网络的设计、关键超参数工程（如探索噪声和学习率）以及训练中的常见失败模式与高级调试策略。
tags:
  - 强化学习
  - DDPG
  - TD3
  - ActorCritic
  - 超参数
  - 0-1先生
categories:
  - AI项目
  - 模型训练
---

### 研讨主题：赋予智能体行动的智慧——DDPG模型实战训练与策略评估

在 **0-1先生** 的监督下，我们成功搭建了高保真海洋模拟器。现在，我们正式进入强化学习（RL）策略的训练阶段。我们的目标是让 **DDPG (Deep Deterministic Policy Gradient)** 或其更稳定的变体 **TD3 (Twin Delayed DDPG)** 学会一套最优的控制策略 ($\pi$)，以最大化我们之前精心设计的**奖励函数**。

本次研讨将围绕三个核心维度展开：**算法选择的深度分析**、**Actor-Critic 网络的结构设计**和**决定训练成败的超参数工程**。

---

### 1. 算法选择的深度分析：DDPG、TD3与连续控制的挑战

#### 1.1 DDPG 的核心优势与固有缺陷

DDPG 是基于 **Actor-Critic** 架构的深度RL算法，专为**连续动作空间**设计。我们的探鱼设备控制（如推进器推力 $0 \sim 100\%$）就是典型的连续控制任务。

* **Actor（策略网络）**：直接输出确定的动作 $a_t = \mu(s_t)$。
* **Critic（价值网络）**：评估 Actor 输出动作的价值 $\text{Q}(s_t, a_t)$。

**缺陷：Q值过高估计（Overestimation）**。DDPG 在更新 Critic 时，使用下一个状态的最佳 Q 值（通过目标网络 $Q'(s_{t+1}, a_{t+1})$ 估计）。这在复杂环境下容易导致 Q 值被系统性高估，从而学习到次优甚至危险的策略。

#### 1.2 升级：为什么我们更倾向于 TD3 (Twin Delayed DDPG)

为了克服 DDPG 的不稳定性，我们决定以 **TD3** 作为我们的主力算法。TD3 引入了三大核心改进，显著提高了训练的鲁棒性和策略的稳定性：

1.  **双 Critic 网络（Twin Critics）**：使用两个独立的 Critic 网络 $Q_1$ 和 $Q_2$ 来估计 Q 值，更新时取两者中**较小的那个**（$\min(Q_1, Q_2)$）。这极大地抑制了 Q 值的过高估计问题。
2.  **延迟策略更新（Delayed Policy Updates）**：Critic 网络更新两次，Actor 网络才更新一次。这是因为 Q 值估计（Critic）需要更稳定、更精确，才能有效地指导策略（Actor）的更新。
3.  **目标策略平滑（Target Policy Smoothing）**：在目标动作上添加**随机裁剪噪声**。这使得 Q 值函数对动作的微小变化不那么敏感，平滑了 Q 函数，减少了方差。

**结论：** 在实际工程应用中，**TD3** 比 DDPG 更稳定、更高效。我们将以 TD3 架构进行训练。

---

### 2. 智能体“大脑”的深度架构设计：Actor-Critic 网络的结构与输入

我们的智能体**状态向量** $S_t$ 是一个复杂的多模态输入，网络设计必须能够高效处理。

#### 2.1 状态向量 $S_t$ 的构建（输入层）

我们必须将来自 YOLO 模型的**感知信息**与来自传感器（GPS/IMU）的**自身信息**进行有效融合。

| 数据来源 | 数据项（特征） | 维度 | 预处理方式 |
| :--- | :--- | :--- | :--- |
| **YOLO 感知结果** | 鱼群的相对坐标（$x, y, z$）、鱼群的边界框大小、海底障碍物的相对最近距离。 | 6 - 9 维 | **归一化（Normalization）**：将坐标和距离缩放到 $[-1, 1]$ 区间。 |
| **设备自身状态** | 航向（Yaw）、俯仰角（Pitch）、速度（Surge）、角速度、水深。 | 5 - 6 维 | **三角函数编码（Periodic Encoding）**：对角度信息（如航向）使用 $\sin(\theta)$ 和 $\cos(\theta)$ 进行编码，避免角度突变带来的梯度问题。 |
| **总状态向量 $S_t$** | **融合后的所有特征** | $\sim 15$ 维 | 统一输入到 Actor 和 Critic 网络的首层。 |

#### 2.2 Actor (策略) 网络架构

Actor 的任务是根据当前状态 $S_t$，输出一个**确定的**最优动作 $A_t$。

* **结构：** 三层全连接网络（MLP）。
    * **输入层：** $S_t$ 向量（15维）。
    * **隐藏层 1 & 2：** 256 或 512 个神经元。使用 **ReLU** (Rectified Linear Unit) 激活函数。
    * **输出层：** 动作向量 $A_t$（例如 3 维：前进推力、左舵推力、探头角度增量）。使用 **Tanh** (Hyperbolic Tangent) 激活函数将输出动作值限制在 $[-1, 1]$ 范围内，以便于映射到实际硬件的控制范围。
* **优化目标：** 最大化 Critic 评估的动作价值 $E[\text{Q}(s_t, \mu(s_t))]$。

#### 2.3 Critic (价值) 网络架构（TD3 双 Critic）

Critic 的任务是评估 Actor 输出的动作的价值。

* **结构：** 同样是三层全连接网络。
    * **输入层：** 状态 $S_t$（15维）与动作 $A_t$（3维）**拼接**在一起，总输入 18 维。
    * **隐藏层 1 & 2：** 256 或 512 个神经元。使用 **ReLU** 激活函数。
    * **输出层：** 单一的 Q 值（标量）。
* **优化目标：** 最小化 Critic 的时序差分（TD）误差，使其输出 $Q(s_t, a_t)$ 尽可能接近目标 $y_t$。

---

### 3. DDPG/TD3 训练的关键超参数工程

超参数的选择对于 RL 算法的收敛速度和最终性能至关重要。

| 超参数 | 推荐值/策略 | 工程意义与调优挑战 |
| :--- | :--- | :--- |
| **学习率 ($\alpha_{actor}, \alpha_{critic}$)** | Actor: $10^{-4}$；Critic: $10^{-3}$ | **0-1先生策略：** Critic 的学习率通常略高于 Actor，以确保价值估计（Critic）能快速且准确地收敛，从而为策略更新（Actor）提供可靠的指导。 |
| **软更新率 ($\tau$)** | $0.005$ | 控制目标网络的更新速度。较低的 $\tau$ 意味着目标网络更新缓慢，提高了稳定性。过高的 $\tau$ 会使训练变得不稳定。 |
| **经验回放缓冲区大小** | $10^6$ (一百万个经验元组) | 存储大量的历史经验，打破数据间的时序相关性（非独立同分布问题），这是离线策略算法稳定性的基石。 |
| **批次大小 (Batch Size)** | $256$ 或 $512$ | 影响梯度更新的方差。较大的 Batch Size 能更准确地估计梯度，但会增加 GPU 内存消耗。 |
| **探索噪声 ($\mathcal{N}_t$)** | 初始标准差 $\sigma=0.3$ | 智能体在训练初期必须有足够的**探索**。我们使用高斯噪声并将其添加到 Actor 的输出动作上 $a_t = \mu(s_t) + \mathcal{N}_t$。随着训练进行，$\sigma$ 必须**线性衰减**至接近 0，以最终实现确定的最优策略。 |
| **折扣因子 ($\gamma$)** | $0.99$ | 决定智能体对未来奖励的重视程度。接近 1（如 0.99）意味着智能体更注重长期规划（如探鱼路径），而非眼前利益。 |

---

### 4. DDPG/TD3 模型在模拟器中的首次运行与评估

#### 4.1 训练流程（伪代码/算法流程）

1.  **初始化：** 初始化 Actor、Critic 及其目标网络，初始化经验回放缓冲区 $\mathcal{R}$。
2.  **探索阶段：** 在训练初期，智能体使用高探索噪声 $\mathcal{N}_t$ 在模拟器中随机行动，收集大量 $(s_t, a_t, r_t, s_{t+1})$ 元组存入 $\mathcal{R}$。
3.  **迭代更新：**
    * 从 $\mathcal{R}$ 中随机抽取 Batch Size 的经验元组。
    * **Critic 更新（两次）：** 使用目标网络和 $\min(Q_1, Q_2)$ 计算目标 $y_t$，最小化 TD 误差。
    * **Actor 更新（延迟更新）：** 延迟更新 Actor 网络，使其最大化 $Q_1$（或 $Q_2$）输出的动作价值。
    * **目标网络软更新：** 使用 $\tau$ 软更新目标网络的权重。
4.  **循环：** 重复步骤 2 和 3，直到平均回合奖励收敛。

#### 4.2 初始政策评估：指标与诊断

我们必须在训练过程中持续监控指标，以诊断智能体的学习状态。

| 评估指标 | 目标期望值 | 诊断意义与行动 |
| :--- | :--- | :--- |
| **平均回合奖励** | 随时间稳定增长，方差减小。 | **核心指标。** 如果停滞不前或波动巨大，表明奖励函数或超参数设置有问题。 |
| **安全碰撞频率** | 迅速下降至接近 0。 | **生命线指标。** 若下降缓慢，说明安全惩罚项 $R_{Safety}$ 权重设置不足，需要大幅增加。 |
| **Critic Loss** | 随时间下降并稳定在一个较低值。 | **模型健康指标。** 如果 Loss 突然飙升，意味着训练发散，需要立即降低学习率或使用**梯度裁剪（Gradient Clipping）**。 |
| **平均 Q 值** | 随时间稳定上升，但不能无限制飙升。 | **价值估计指标。** 监测 Q 值是否过高估计（尤其是在 DDPG 中），这可能导致智能体过于自信并采取危险动作。 |

---

### 5. 高级调试与训练失败模式的应对策略

在复杂的 RL 训练中，失败是常态。**0-1先生** 为我们总结了两种主要的训练失败模式及其解决方案：

#### 失败模式 I：策略发散（Policy Divergence）

* **现象：** 损失函数在训练中途突然暴涨，平均奖励急剧下降，智能体行为变得随机或极端。
* **根本原因：** 学习率过高，导致网络权重更新过于激进；或者目标网络更新过快。
* **解决方案：**
    * **降低学习率：** 将 $\alpha_{actor}$ 和 $\alpha_{critic}$ 至少降低一个数量级（例如从 $10^{-3}$ 降到 $10^{-4}$）。
    * **梯度裁剪（Gradient Clipping）：** 限制梯度的最大范数，防止单个批次数据产生过大的梯度更新。
    * **增加 $\tau$：** 进一步减慢目标网络的软更新率。

#### 失败模式 II：局部最优或“懒惰”策略

* **现象：** 智能体在某个较低的奖励水平上收敛，策略稳定但未能达到最优。例如，它学会了安全地待在原地，避免所有风险，但也没有找到鱼群。
* **根本原因：** 探索不足；或者奖励函数设计过于保守（安全惩罚权重过高）。
* **解决方案：**
    * **增加探索：** 延长探索噪声的衰减周期，或提高初始探索噪声 $\sigma$。
    * **奖励塑造（Reward Shaping）：** 调整奖励函数的权重，尤其是增加 $R_{Attraction}$ 的权重，或引入一个微小的**“运动奖励”**，鼓励智能体主动探索。
    * **课程学习（Curriculum Learning）：** 先在只有鱼群、没有障碍物的简单环境下训练，稳定策略后，再逐步引入复杂障碍物和水流。

### 6. 阶段总结与下一步计划：向硬件迈进

通过这次对 **TD3 深度架构和超参数工程**的研讨，我们已经为智能体的“大脑”打下了坚实的基础。我们将在高保真模拟器中启动训练，并严格监控上述性能指标和失败模式。

一旦智能体在模拟器中表现出高度的鲁棒性（安全碰撞频率低于 $1\%$），我们将正式进入下一阶段，也是最激动人心的阶段：**硬件集成与控制**。届时，我们将把训练好的 YOLO 模型和 TD3 策略部署到边缘计算单元，并建立与探鱼设备的通信协议。

---

